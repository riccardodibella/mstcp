Domande:
- Chiedo per sicurezza... per il congestion control prendo come base quello che viene fatto in mytcp e lo modifico solo dove necessario
per adattarlo alla struttura del mio programma (ad esempio spostare rtt_estimate), giusto? Ci sono altre correzioni da fare o bug noti nel
funzionamento di quel meccanismo in mytcp?
- In mytcp, per controllare che un pacchetto sia un DupACK si controlla che sia solo un ACK, che abbia payload 0, che il numero di ACK
sia uguale al precedente, e che la sua window sia uguale a quella precedente. Dato che con MS-TCP la window non viene mai inviata con segmenti
di lunghezza 0 (perchè per l'aggiornamento della advertised window si usa un segmento DMP), posso togliere la condizione sulla finestra che
non è stata modificata? Anche perchè dato che quel pacchetto con lunghezza 0 non avrà un SID non so neanche a quale entry di radwin dovrei fare
riferimento
- I segmenti DMP non vengono considerati per la advertised window. Devono essere ignorati anche per il calcolo della congestion window? Quindi
sia per evitare un update della congestion window quando arriva un ACK, sia per ignorarli quando si calcola il numero di byte in volo durante
le ritrasmissioni
- Se non ho capito male, in mytcp viene considerato che un pacchetto "pieno" porta 1MSS byte. Nel nostro caso però non è così: se trasmettiamo
un MSS con la sua estensione, noi in pratica trasmetteremo segmenti da 1MSS - 40B, per la dimensione fissa delle opzioni. In questo modo per il
congestion control c'è un mismatch continuo tra quello che si può inviare in più dopo un incremento di 1MSS e quello che si può inviare in un
segmento singolo. Come dovrei gestire questa cosa?
- Bisogna cambiare il modo di aggiornare l'RTT adesso che si considerano più misure per ogni ciclo di RTT invece
che solo una? Avevo letto qualche discussione a riguardo, ma non ho trovato nessuna indicazione precisa
- Perchè in mytcp in myio si imposta un accumulatore (tot) usando totlen (cioè 20+optlen+payloadlen) invece che usare payloadlen?
- La cwmd va considerata anche nello scheduler per evitare di avere una coda troppo lunga di segmenti non acked per uno stream e permettere
di aggiungere segmenti DMP che verranno trasmessi in tempi brevi, giusto? Quindi la cwnd verrà considerata sia in mytimer sia nello scheduler,
corretto?
- Probabilmente è una domanda stupida, ma a cosa serve precisamente il controllo fdinfo[s].tcb->st == TCB_ST_CLOSED in mywrite? Anche
perchè in mytcp dopo ci si comporta allo stesso modo in ogni caso. Io dovrei controllare lo stato dello stream, non del TCB, giusto?





Risposte:
- Se devo mandare un ACK senza payload, lo associo allo stream 0?
    => NO, non inserire proprio l'opzione MS, in questo modo non si consuma un segment sequence number, che va bene
    dato che non si sta facendo avanzare il sequence number e quindi non ci interessa avere un ACK dell'ACK che si
    sta inviando
- La stima iniziale dell'RTT (lato active open) la faccio già considerando il SYN+ACK, giusto?
    => Probabilmente no, ma serve leggere sull'RFC (RFC 6298)
- Una volta che ho una sequenza di misure dell'RTT, come si calcola la stima dell'RTT da usare per il timeout?
    => Stesso meccanismo di mytcp, con rtt_e e Drtt_e (RFC 6298)
- Non devo tenere nessuno stato relativo ai SACK ricevuti, giusto? Quando ricevo un SACK libero lo spazio di quel 
pacchetto dalla flightsize dello stream corrispondente (e evito di ritrasmetterlo), ma è inutile salvare una "lista
di SACK ricevuti dopo il cumulative ACK" giusto?
    => Esatto
- A quanto avevamo detto, in MS c'è uno scheduler che man mano che appena si libera spazio nella congestion window
cerca di riempirlo con i dati dai tx buffer di ogni stream. Questo meccanismo del tx buffer non c'era nel tcp normale,
in cui i dati venivano segmentati direttamente ed inseriti subito nella tx queue. Metto lo stesso meccanismo anche nel
TCP normale, quindi introduco anche lì un TX buffer (che sarà associato solo allo stream 0 invece di essere per ogni
stream), oppure lascio così com'è e non creo nessun TX buffer per tcp normale?
    => Va bene inserire lo stesso meccanismo con un buffer e un "consumatore" (equivalente allo scheduler di MS-TCP) 
    anche in TCP normale
- Mi serve qualcosa di diverso da STREAM_STATE_CLOSED per gestire il broken pipe per backlog piena?
    => Credo basti STREAM_STATE_CLOSED
- Nel paper è scritto, per l'apertura dei nuovi stream, di aspettare un timeout, e se non vengono inviati dati
entro quel timeout aprire lo stream con un pacchetto con il flag dummy payload. Faccio comunque ritornare connect() 
subito, senza aspettare di avere lo stream aperto, giusto? Sennò non posso inviare dati se sono fermo dentro connect.
    => Giusto, myconnect() ritorna subito
- Se non sbaglio, al momento il retransmission timer può andare da 300ms a 2000 tick (10s). Da dove arrivano
questi valori?
    => Lo spirito è quello di altri valori che fanno la stessa cosa presenti negli RFC, questi sono più adatti
    all'Internet di oggi, ma non so comunque da dove vengano di preciso
- Sempre riguardo al timer dell'apertura degli stream, ho aggiunto un campo stream_fsm_timer ad ogni stream,
che viene usato per salvarmi a che valore di "tick" devo inviare il pacchetto con flag dummy payload se non è
ancora stato inviato nessun dato. Probabilmente può essere accorpato in qualche modo con il timer di canale usato
per la chiusura della connessione, ma finchè non faccio quella parte è più comodo tenerli separati. Ha senso?
    => Sì va bene
- A cosa serve l'ACK che viene inviato quando viene fatto myaccept?
    => Forse a niente, ma non dà fastidio, può rimanere lì
- Devo capire cosa fare con i TCB dei passive open... Al momento è facile: quando faccio accept libero il posto
nella backlog e ho una copia di quel TCB linkata solo in fdinfo per quel socket. Posso tenere la stessa cosa
anche adesso nelle connessioni non-MS. Per le connessioni MS, non posso togliere il TCB dalla backlog
fino a quando non ho riempito tutti e 32 gli stream (ma per consistenza non lo toglierei proprio mai dalla
backlog finchè la connessione non è chiusa). Quando faccio accept(), linko in fdinfo[s] lo stesso TCB.
Quando faccio la chiusura di uno stream, so che non devo chiudere la connessione a meno che quando mi arriva
l'ACK della mia chiusura dello stream non siano stati chiusi gli stream, in quel caso posso chiuderla.
Quando chiudo la connessione cancello anche la entry dalla backlog.
    => Abbiamo cambiato la struttura della backlog: adesso ha una dimensione fissa, e contiene solo le connessioni
    nuove di cui dobbiamo fare ancora accept, non gli stream. Per questo problema specifico dell'ownership dei TCB
    farei così: quando chiudo uno stream e basta (se c'è almeno un altro stream in stato OPEN), so che qualcun altro
    chiuderà la connessione, e posso disinteressarmi di quello che succederà al canale da quel punto in poi. Invece, 
    se sono l'ultimo stream che è OPEN, mando il mio segmento per chiudere il mio stream e inizio la chiusura del
    canale. Questa procedura per la chiusura andrà capita meglio più avanti
- Sicuramente è una domanda abbastanza stupida, ma chiedo conferma... Quando arriva un pacchetto per un TCB che è
nella backlog (già established), ignoro i pacchetti che gli arrivano, giusto?
    => Esatto, per il momento va bene così
- Per aggiungere un nuovo stream alla coda degli stream pronti, nel TCB che avevo associato ad un nuovo socket con
accept() mi servirebbe il fd del listening socket, in cui trovo la backlog degli stream. Al momento lo ho aggiunto,
lascio così o cerco il fd giusto scrorrendoli tutti confrontando local address local port e stato (LISTEN o SYN_RECEIVED)?
    => Va bene così, a rigore bisgnerebbe anche porsi il problema di fare il RST degi stream ready quando viene fatto
    il cose di un listening socket
- Nel campo "sequence" del txcontrolbuf, si scrive la sequenza includendo il sequence offset casuale, giusto?
    => Si va bene così, se funziona tutto è ok
- Finchè faccio queste prove, considero anche per le connessioni TCP normali che il SACK di un segmento non può essere 
"annullato" (così evito di avere il codice di gestione diverso) oppure li gestisco già diversamente? Sennò dovrei
togliere i pacchetti dalla TX queue per MS-TCP, e invece mettere un flag che mi dice di non ritrasmettere ma lasciarli
comunque lì per i TCP normali.
    => Va bene così per il momento
- E' giusto che, per sapere quando devo inserire i dati di un segmento che mi arriva su uno stream all'interno del 
RX buffer per quello stream, io debba aggiungere al TCB per ogni stream qual'è il prossimo SSN che devo inserire nel
buffer per quello stream? E devo anche salvare in quale posizione inserirlo, perchè non posso più usare il numero di
sequenza del segmento e l'offset iniziale per determinare la posizione nel buffer in cui scrivere il nuovo dato
    => Aggiungere solo il numero del prossimo segmento da aggiungere alla linked list di stream, dato che non esiste un
    buffer circolare in cui sono inseriti i numeri dei payload non serve l'indice della prossima posizione in cui scrivere
- Nel TCB c'è un campo txfree (con un valore per ogni stream), questo indica lo spazio nella advertised window per
quello stream che non è ancora stato riempito con pacchetti già in volo di cui si aspetta l'ACK, giusto? Perchè in
mytcp è iniziaizzato con un valore fisso (TXBUFSIZE), non capisco perchè.
    => 29/03 non mi ricordo se mi ha risposto o no, in teoria l'inizializzazione a TXBUFSIZE dovrebbe essere sbagliata
- Non mi ricordo se lo avevo già chiesto, ma se MS-TCP è abilitato, ha senso usare il flag dummy payload anche per
segmenti che non hanno l'opzione MS-TCP? O dato che senza l'opzione MS-TCP non ho bisogno di consumare un nuovo SSN
per poterci fare l'ACK e quindi non mi serve mai il flag DMP? E' solo per capire dove mettere il controllo per quel
flag, se farlo anche per segmenti di una connessione con MS-TCP abilitato ma senza l'opzione MS-TCP all'interno
    => 29/03 non ricordo da dove uscisse questo dubbio, ma non ha senso usare DMP per pacchetti che non consumano SSN
- Non ho ancora iniziato a fare la parte di ricezione, ma vorrei capire se è giusto come penso di impostarla:
    - nel paper viene messo solo un buffer di trasmissione (consumato dallo scheduler), non un buffer di ricezione
    in cui inserire i dati quando arrivano. A seconda di se si voglia aggiungere questo buffer di ricezione o meno
    ci sono due casi possibili:
    - se si vuole aggiungere il buffer di ricezione: 
        - c'è un buffer di ricezione diverso per ogni stream
        - dato che il flusso dei seq number è associato alla connessione intera, non posso usare quello per inserire
        "a colpo sicuro" i dati nel buffer di ricezione
        - se arriva un segmento con un SSN immediatamente successivo all'ultimo ricevuto, lo aggiungo all'inizio dello
        spazio libero nel buffer dello stream
        - se un segmento ha il flag dummy payload, non lo aggiungo mai al buffer
        - in questo caso, la advertised window è uguale allo spazio libero nel buffer per lo stream
    - se non si vuole mettere il buffer in ricezione:
        - c'è una linked list per il canale, che contiene i pacchetti ricevuti out-of-order, in cui rimangono fino a
        che il buco nel sequence number non viene riempito da un altro segmento
        - c'è una linked list per ogni stream, con i segmenti per quello stream il cui payload deve ancora essere
        consumato completamente
        - quando arriva un nuovo segmento, oltre a inserirlo nella linked list di canale, se il suo SSN è immediatamente
        successivo all'ultimo ricevuto per quello stream, lo inserisco alla fine della linked list di stream, con NULL
        come nodo successivo
        - dopo avere inserito un nodo (per la sua ricezione o per un segmento precedente che ha riempito il buco degli SSN)
        nella linked list di stream, percorro la linked list di canale per cercare il primo segmento per lo stesso stream;
        se il SSN è immediatamente successivo a quello appena inserito inserisco anche quel segmento, e ripeto la procedura
        fino a che il primo segmento non è immediatamente successivo o arrivo alla fine della channel rx linked list senza
        trovare altri segmenti sullo stream
        - ogni nodo della linked list di stream (che contengono il payload dei segmenti) ha un campo che mi dice quanti byte
        di payload ho già consumato con read(), quando questo valore arriva alla lunghezza del payload del segmento lo tolgo
        dalla lista, e tolgo dalla lista anche i segmenti all'inizio che hanno il flag dummy payload
    => Non esiste il "buffer" di ricezione, c'è una linked list di canale unica, e poi una linked list per ogni stream
- Abbiamo detto che ha senso introdurre un buffer di trasmissione, da cui i dati vengono consumati da qualcosa di
analogo allo scheduler del Multi-Stream (ma più semplice), anche per TCP normale. Come dimensiono questo buffer?
Metto una dimensione fissa uguale per TX buffer e RX buffer per ogni stream o faccio qualcosa di diverso? Prima
c'erano i due parametri TXBUFSIZE e RXBUFSIZE, ma il primo non veniva usato effettivamente per la dimensione di
un buffer, solo per inizializzare il campo txfree dei TCB.
    => 29/03 Per il momento TCP normale lo sto lasciando da parte quindi il problema non si pone
- Anche per quanto riguarda TCP normale (non solo multi-stream), gli ACK che non hanno nessun payload vanno ritrasmessi se il
loro timer di ritrasmissione scade?
    => No
- Ha senso inizialmente tenere una dimensione del "buffer" (virtuale, in realtà è una linked list) di ricezione molto bassa per
verificare che il flow control e la modifica della advertised window funzionino correttamente? In questo modo posso iniziare a
lavorare sullo scheduler senza pensare al congestion control
    => Sì va bene
- Per il momento può andare bene se faccio tutto ignorando completamente possibili problemi di "Silly Window Syndrome"? Eventuali
protezioni a riguardo si possono aggiungere dopo, giusto?
    => Sì
- Dimensionamento del buffer (virtuale) di ricezione, cioè sostanzialmente della advertised window: secondo quanto avevamo detto,
dobbiamo mettere un valore fisso "RXBUFSIZE", che corrisponde alla dimensione massima del payload non consumato nella RX queue di
stream. Il valore deve essere fisso perchè sennò non si sa cosa impostare come valore iniziale per la creazione "unsolicited" dei
nuovi stream successivi al primo. Successivamente non dovrebbe esserci problema ad "alzare" questo limite dopo che lo stream è
stato creato, ma quello iniziale deve essere fisso. Una volta che il limite viene alzato non può più essere abbassato, perchè
sennò quando si "rifiuta" un pacchetto che va oltre alla nuova advertised window ridotta non si può più fare l'ACK dei segmenti
degli altri stream successivi a quello rifiutato. Ho capito giusto?
    => Sì, con MS-TCP va disabilitata la possibilità di abbassare la advertised window per evitare questo genere di mal di testa
- La advertised window (proprietà di stream) va calcolata basandosi sul cumulative ACK (proprietà di canale). Questo è un
problema perchè la mia intenzione era quella di aggiornare la advertised window ogni volta che prendevo un segmento e lo spostavo dalla
RX queue di canale alla RX queue di stream, e ogni volta che consumo qualcosa dalla RX queue di stream. Però non posso fare così (o almeno
non solo), perchè il valore della advertised window cambia anche ogni volta che il cumulative ack "passa oltre" uno dei segmenti trasmessi
per quello stream. Secondo me ci sono due alternative: o salvare un valore della advertised window da mettere direttamente nei segmenti di
ogni stream, però vanno aggiornati i valori per tutti gli stream aperti ogni volta che cumulativeack avanza, oppure si potrebbe ricalcolare
ogni volta facendo la differenza tra il valore "massimo" (RX_VIRTUAL_BUFFER_SIZE) e lo riempimento del buffer virtuale calcolato scorrendo
tutta la coda di segmenti pronti da essere consumati, considerando solo quelli che hanno un sequence number più basso di cumulativeack (con
riferimento all'offset iniziale). 
    => Dovrebbe andare bene salvarla nel tcb e cambiarla ogni volta che serve
- La advertised window ricevuta va aggiornata ogni volta che il cumulative ack avanza oltre un segmento di uno stream, giusto?
    => Giusto
- Penso di sì, ma per sicurezza chiedo... Se consumo metà del payload di un segmento nella RX queue di stream, devo aumentare la
adwin per quello stream del numero di byte che ho consumato, anche se non ho ancora liberato la memoria per quel segmento (che ha
ancora dei byte da consumare) giusto?
    => Giusto
- Gestione del flag DMP per la advertised window: Al ricevitore non viene contato il fatto che usano spazio (quindi quando vengono
ACKed il loro payload non viene contato come "spazio occupato"), però il trasmettitore lì conta comunque come spazio occupato (1 byte)
all'interno della window che è in-flight. Ho capito giusto? Devo fare attenzione a qualcos'altro?
    => NO! I segmenti DMP non consumano mai spazio nella advertised window
- A rigore, dato che li sto considerando per la adwin in ricezione, dovrei contare che i pacchetti con flag DMP occupano un byte
nella window anche al trasmettitore, giusto? In questo caso però, se devo mandare un ACK DMP potrei andare oltre la adwin del
ricevitore, quindi forse serve prevedere un po' di margine garantito per permettere lo scambio di segmenti DMP anche quando la
finestra è satura. Poter mandare segmenti DMP è necessario, perchè non si possono mandare aggiornamenti della advertised window 
senza specificare lo stream, e se non ci sono dati in uscita serve per forza il flag DMP
    => I segmenti DMP non occupano spazio nella advertised window
- In TCP normale era facile capire se un segmento cadeva all'interno del RX buffer, perchè il sequence number corrispondeva a una
posizione precisa (relativa all'inizio del buffer circolare). Adesso questa cosa non si può più fare: da un lato non ci interessa
più posizionare i segmenti direttamente in un buffer perchè c'è una linked list, dall'altro però è più difficile (/impossibile)
capire se un segmento cade dentro o fuori dalla advertised window. Per il momento sto ignorando il controllo di se i segmenti
sono all'interno della advertised window, ma (molto più avanti) come si può fare?
    => Per il momento lo si può ignorare, ma potenzialmente con MS-TCP ci può essere qualche vantaggio che deriva dal fatto di
    avere un pool di memoria unico tra tutti gli stream, bisognerà pensarci meglio
- E' giusto che io pensi allo scheduler in principio come una funzione stateless, da chiamare ogni volta che c'è la possibilità di
poter forse trasmettere qualcosa di nuovo, e lui valuta lo stato del TCB per capire cosa fare? Quindi le chiamate dovrebbero essere
idempotenti, e chiamare lo scheduler due volte di fila non dovrebbe fare niente (a parte sprecare tempo)
    => In testa mia è giusto, volendo lo si può vedere diversamente come una macchina a stati, in cui oltre che il tcb gli si passa
    l'evento che è successo, questo permetterebbe di fare molte meno operazioni ad ogni chiamata

Fonti:
https://www.rfc-editor.org/rfc/rfc7323#section-4 Spiegazione di quali misure dell'RTT vanno usate per l'aggiornamento

Problemi di MYTCP:
- Il bind automatico in mybind di mytcp non fa htons() per salvare in fdinfo[s].l_port
- Problema con il timer per il timeout se non si specifica il timeout iniziale in mytcp
- Il ciclo che toglie i pacchetti di cui si è ricevuto l'ack toglie l'ACK del 3-way handshake
- Viene accettata solo una connessione alla volta
- In myaccept, viene dato errore se lo stato del TCB è SYN_RECEIVED, anche se c'è il toggle tra i due stati
- In myio, dopo aver chiamato la FSM, se la connessione non è established viene fatto "break" invece di "continue"